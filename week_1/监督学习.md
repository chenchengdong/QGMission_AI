# 监督学习

**分类问题 回归问题**

**线性回归**

![](D:\typora\typora_note\线性回归.png)

![](D:\typora\typora_note\标识符.png)



![](D:\typora\typora_note\回归函数.png)



![](D:\typora\typora_note\线性函数.png)

**平方误差/平方差函数/代价函数**

![](D:\typora\typora_note\代价函数.png)

![](D:\typora\typora_note\代价函数一.png)

 

**代价函数（二）**

- “凸函数”---一般线性回归的代价函数都长这样，只有一个全局最优位置

![](D:\typora\typora_note\代价函数二.png)

- 存在局部最优

![](D:\typora\typora_note\局部最优.png)





![](D:\typora\typora_note\bowl_fun.png)



### 梯度下降

![](D:\typora\typora_note\downstairs.png)



![同步更新、梯度下贱](D:\typora\typora_note\梯度下降函数.png)

- α是指学习效率，α越大梯度下降速率越快，控制我们以多大的速率更新θ0和θ1

- 分别从局部最优解左右趋近

![](D:\typora\typora_note\分别从局部最优解左右趋近.png)



- 如果α很小或很大，有可能要很久才能到达局部最优解或者达不到（发散）

![](D:\typora\typora_note\学习效率大小.png)

- 如果一开是初始化的θ1就位于局部最优解处，会不动

![](D:\typora\typora_note\开始位于局部最优解处.png)



**线性回归模型**

![](D:\typora\typora_note\线性回归算法.png)

- 偏导数得到的θ0和θ1

![](D:\typora\typora_note\偏导数得到的.png)

- batch算法-----这种一次遍历所有数据的梯度下降算法（上学）
- 如果θ1离得较远时，接近最优解的步伐会由大到小慢慢靠近

![](D:\typora\typora_note\小步靠近.png)





### 矩阵和向量 

![](D:\typora\typora_note\矩阵.png)

- 矩阵元素

![](D:\typora\typora_note\矩阵元素.png)

- 向量与矩阵的一般表示

![](D:\typora\typora_note\向量和矩阵一般表示.png)





- 矩阵与数的加减乘除运算

![](D:\typora\typora_note\矩阵与数加减乘除运算.png)

- 矩阵与向量相乘

![](D:\typora\typora_note\矩阵与向量相乘.png)

- 矩阵与矩阵相乘

![](D:\typora\typora_note\矩阵与矩阵相乘.png)

- 矩阵与矩阵乘法的特征：
  1. 矩阵与矩阵相乘不符合交换律
  2. 矩阵与矩阵相乘符合结合律



- 单位矩阵

![](D:\typora\typora_note\单位矩阵.png)

- 逆和转置

  1. 只有方阵才有逆矩阵，全是0的矩阵也没有逆矩阵（奇异矩阵\退化矩阵）

  2. 转置：行变列   2*3->3*2

     ![](D:\typora\typora_note\矩阵转置.png)



- 多特征量表示方法

![](D:\typora\typora_note\多特征量.png)



- 多元线性回归
  1. 可以加一个x0参数令其等于1，得到一个更紧凑的表达结果
  2. 表示方式

![](D:\typora\typora_note\多元线性回归.png)

- 多元代价函数有些θ的表示方法出了点改变

![](D:\typora\typora_note\多元线性回归改变.png)



- 特征缩放（一般运用在代价函数上）

  1. 如果有些特征值很大，有些很小，那么在接近局部最优解的时候路劲会比较长或者来回震荡，这时候我们就可以将他们特征缩小，到一个大约为[ -1, 1]的区间内，这样他们的代价函数会比较像碗状，梯度下降时会比较快
  2. 对x1，x2重新定义？？

  ![](D:\typora\typora_note\定义特征缩放.png)

- 一些可以接受的定义缩放区间

![](D:\typora\typora_note\缩放区间.png)



- 有时，也可以用均值归一化----即每个样本的特征值减去该训练集的平均值的差再除区间（最大值）

![](D:\typora\typora_note\均值归一化.png)



### 学习效率α

- 代价函数与迭代次数不正常---像这种一直往上升的说明学习效率α过大，应往小的调，但是太大又需要很长时间才能收敛（达到最小值）

![](D:\typora\typora_note\迭代学习效率.png)

- 所以一般需要画出代价函数随着迭代次数的曲线判断
- 如何确定学习效率：

![](D:\typora\typora_note\确定学习效率.png)



### 特征和多项式回归

- 当有多个特征量时可以适当合并

![](D:\typora\typora_note\多项式回归.png)

- 当只有一个特征量时可以将其化为多个

![](D:\typora\typora_note\多项式回归2.png)



### 正规方程（区别于迭代方法直接求解）

1. 当使用正规方程的时候可以不需要缩放特征量
2. 正规方程利用矩阵一步求解最优θ法

![](D:\typora\typora_note\正规方程利用矩阵求解.png)



- 正规方程加矩阵

![](D:\typora\typora_note\正规方程加矩阵.png)

- 正规和梯度下降的优缺点

![](D:\typora\typora_note\正规和梯度下降的优缺点.png)



- 正则化进阶：

  1. 如果两个特征量是线性相关的，可以考虑删掉其中一个，然后重新构建剩下的那一个（一般出现在你矩阵不可逆情况下）
  2. 观察是否有多余的特征量
  3. 当训练集m较少而特征量n较多时，可以看看删掉一些不影响或者影响小的特征量
  4. 最后才考虑正规化的方法（后面讲）

  ![](D:\typora\typora_note\正.png)



- 矢量 

![](D:\typora\typora_note\求和可以看成向量相乘.png)

- 分类（正类和负类），逻辑回归--------分类算法 

- 可能有用的图片：

![](D:\typora\typora_note\一些看起来可能有用的图片.png)



- 决策界限

![](D:\typora\typora_note\逻辑回归判断条件.png)

- 决策边界

![](D:\typora\typora_note\决策边界.png)

- 注：决策边界不是训练集的属性，而是假设本身及其参数的属性，训练集是我们用来找到最拟合的θ

![](D:\typora\typora_note\边界.png)

**逻辑回归--代价函数**

- 如果用线性回归的代价函数来拟合逻辑回归会得到如下结果

![](D:\typora\typora_note\重新表示代价函数的形式.png)



- 用于拟合逻辑回归的代价函数：

![](D:\typora\typora_note\逻辑代价函数.png)

![](D:\typora\typora_note\逻辑代价函数2.png)

- 简化代价函数和梯度下降

- 这时候代价函数变了，一个cost形式的是针对单个训练集的，另一个J形式是针对全体训练集的

![](D:\typora\typora_note\逻辑代紧.png)

- 逻辑回归拟合θ---梯度下降算法，要区别前面的线性回归，这里的假设函数不一样，之前的特征缩小方法思想也可以运用到逻辑回归中，也可以用向量来一次更新全部θ，看你自己理解

![](D:\typora\typora_note\逻辑回归梯度下降.png)



- 高级优化

![](D:\typora\typora_note\emmmm.png)

- 多元分类------1对多：

- 过度拟合：

  1. 欠拟合（高偏差）

  ![](D:\typora\typora_note\欠拟合_回归.png)

  1. 过拟合（高方差） ---这种虽然拟合度很高，但是由于过度追求拟合完整度，导致曲线跌宕曲折，这种曲线一般不能拿来预测新的值

     ![](D:\typora\typora_note\过拟合_逻辑回归.png)

- 解决国度拟合的办法：
  1. 减少选取变量的数量：人工检查特征量，择重要的留之，但是可能会损失掉一些关键分析某个问题的数据
  2. 正则化：保留所有特征量，但是将θ减少（下节）



- 7-2正则化

![](D:\typora\typora_note\正则化2.png)
